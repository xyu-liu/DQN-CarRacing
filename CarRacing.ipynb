{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8SgwWzf35sP",
        "outputId": "ee1993f4-7965-446b-8918-1ae09f7804a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q swig\n",
        "!pip install -q gymnasium[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yH5TiRQw37TX",
        "outputId": "1c2839c0-877c-4d0d-e273-a2d5bcf9d085"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random \n",
        "from collections import namedtuple, deque \n",
        "import gymnasium as gym"
      ],
      "metadata": {
        "id": "h1JOamW6QfPp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CarRacing-v2\",continuous=False)"
      ],
      "metadata": {
        "id": "KJhGKD6wQPM_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGoKQFYqRI6f",
        "outputId": "d0e62961-f36a-49db-b63c-0e73de0d9cec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Box(0, 255, (96, 96, 3), uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90T_ouTIRc5S",
        "outputId": "5c87cd39-c366-4ac1-ebc0-6bba21043976"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discrete(5)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "k69HvlBZLgqH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(QNetwork,self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 8, kernel_size=4, stride=2) # 3 * 96 * 96\n",
        "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=2) # 32 * 47 * 47\n",
        "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, stride=2) # 64 * 22 * 22\n",
        "        self.conv4 = nn.Conv2d(32, 64, kernel_size=3, stride=2) # 128 * 10 * 10\n",
        "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, stride=1) # 256 * 6 * 6\n",
        "        self.conv6 = nn.Conv2d(128, 256, kernel_size=3, stride=1) # 256 * 6 * 6\n",
        "\n",
        "        self.fc1= nn.Linear(256 * 1 * 1, 100) \n",
        "        self.fc2 = nn.Linear(100,100)\n",
        "        self.fc3 = nn.Linear(100,5)\n",
        "        self.float()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.permute(x, (0, 3, 1, 2))\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = F.relu(self.conv6(x))\n",
        "        x = x.reshape(-1, 256 * 1 * 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)    "
      ],
      "metadata": {
        "id": "bB5jAiffRfX-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "    def __init__(self):\n",
        "        self.qnetwork_local = QNetwork().to(device)\n",
        "        self.qnetwork_target = QNetwork().to(device)\n",
        "        self.memory = deque(maxlen=10000) \n",
        "        self.gamma = 0.97    # discount rate\n",
        "        self.epsilon = 1.0  # exploration rate\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.9999\n",
        "        self.batch_size = 512\n",
        "        self.train_start = 3000\n",
        "        \n",
        "        self.counter_1 = 0\n",
        "        self.counter_2 = 0\n",
        "\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(),lr=0.001)\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "    \n",
        "    def act(self, state):\n",
        "\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        sample = random.random()\n",
        "        if sample > self.epsilon:\n",
        "            action = np.argmax(action_values.cpu().data.numpy())\n",
        "            return action\n",
        "        else:\n",
        "            action =  random.choice(np.arange(5))\n",
        "            return action\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.remember(state, action, reward, next_state, done)\n",
        "        self.counter_2 = (self.counter_2+1) % 500\n",
        "        self.counter_1 = (self.counter_1+1) % 4\n",
        "\n",
        "        if self.counter_2 == 0:\n",
        "            self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "\n",
        "        if self.counter_1 == 0 and len(self.memory) >= self.train_start:\n",
        "            if self.epsilon > self.epsilon_min:\n",
        "                self.epsilon *= self.epsilon_decay\n",
        "            minibatch = random.sample(self.memory, min(len(self.memory), self.batch_size))\n",
        "            self.learn(minibatch)\n",
        "    \n",
        "    def learn(self, batch):\n",
        "\n",
        "        criterion = torch.nn.MSELoss()\n",
        "\n",
        "        states =  np.zeros((self.batch_size, 96, 96 ,3))\n",
        "        next_states =  np.zeros((self.batch_size, 96, 96 ,3))\n",
        "        actions, rewards, dones = [], [], []\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            state_i, action_i, reward_i, next_state_i, done_i = batch[i]\n",
        "            states[i] = state_i\n",
        "            next_states[i] = next_state_i  \n",
        "            actions.append(action_i)\n",
        "            rewards.append(reward_i)\n",
        "            dones.append(done_i)\n",
        "        \n",
        "\n",
        "        actions = np.vstack(actions).astype(np.int)\n",
        "        actions = torch.from_numpy(actions).to(device)\n",
        "\n",
        "        rewards = np.vstack(rewards).astype(np.float)\n",
        "        rewards = torch.from_numpy(rewards).to(device)\n",
        "\n",
        "        dones = np.vstack(dones).astype(np.int)\n",
        "        dones = torch.from_numpy(dones).to(device)\n",
        "\n",
        "\n",
        "\n",
        "        self.qnetwork_local.train()\n",
        "        self.qnetwork_target.eval()\n",
        "\n",
        "        predictions = self.qnetwork_local(torch.from_numpy(states).float().to(device)).gather(1,actions)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            q_next = self.qnetwork_target(torch.from_numpy(next_states).float().to(device)).detach().max(1)[0].unsqueeze(1)\n",
        "        \n",
        "        targets = rewards + (self.gamma * q_next * (1-dones))\n",
        "        targets = targets.float()\n",
        "        loss = criterion(predictions,targets).to(device)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2fnyxiAhbaoR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent()\n",
        "rewards = [] \n",
        "total_episodes = 1500\n",
        "max_steps = 1000\n",
        "\n",
        "for episode in range(total_episodes):\n",
        "    state, _ = env.reset()\n",
        "    cumulative_reward = 0 \n",
        "    \n",
        "    for i in range(max_steps):\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        agent.step(state,action,reward,next_state,done)\n",
        "        state = next_state\n",
        "        cumulative_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    rewards.append(cumulative_reward)\n",
        "    print(f\"Episode {episode}/{total_episodes}, Return = {cumulative_reward}, The epsilon now is : {agent.epsilon}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcvim2_3MBOb",
        "outputId": "8d2e650d-aed1-4030-9440-f9e35ea1dc49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0/1500, Return = -49.64028776978498, The epsilon now is : 1.0\n",
            "Episode 1/1500, Return = -54.71698113207625, The epsilon now is : 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-5ae87425a46e>:68: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  actions = np.vstack(actions).astype(np.int)\n",
            "<ipython-input-13-5ae87425a46e>:71: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  rewards = np.vstack(rewards).astype(np.float)\n",
            "<ipython-input-13-5ae87425a46e>:74: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dones = np.vstack(dones).astype(np.int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 2/1500, Return = -54.69255663430503, The epsilon now is : 0.9999\n",
            "Episode 3/1500, Return = -59.45945945946033, The epsilon now is : 0.9752111619411448\n",
            "Episode 4/1500, Return = -58.1881533101053, The epsilon now is : 0.9511319235669539\n",
            "Episode 5/1500, Return = -50.57034220532397, The epsilon now is : 0.9276472330643509\n",
            "Episode 6/1500, Return = -46.93877551020477, The epsilon now is : 0.9047424102692004\n",
            "Episode 7/1500, Return = -49.82078853046672, The epsilon now is : 0.8824031374898074\n",
            "Episode 8/1500, Return = -54.22535211267692, The epsilon now is : 0.8606154505570021\n",
            "Episode 9/1500, Return = -40.00000000000043, The epsilon now is : 0.8393657300952052\n",
            "Episode 10/1500, Return = -39.50177935943135, The epsilon now is : 0.8186406930090225\n",
            "Episode 11/1500, Return = -34.42622950819699, The epsilon now is : 0.7984273841800505\n",
            "Episode 12/1500, Return = -29.765886287625584, The epsilon now is : 0.7787131683686925\n",
            "Episode 13/1500, Return = -28.571428571428658, The epsilon now is : 0.7594857223159339\n",
            "Episode 14/1500, Return = -23.357664233576642, The epsilon now is : 0.7407330270401349\n",
            "Episode 15/1500, Return = -20.86330935251789, The epsilon now is : 0.7224433603240242\n",
            "Episode 16/1500, Return = -38.95348837209327, The epsilon now is : 0.7046052893871948\n",
            "Episode 17/1500, Return = -31.740614334470987, The epsilon now is : 0.6872076637395368\n",
            "Episode 18/1500, Return = -31.81818181818235, The epsilon now is : 0.6702396082111141\n",
            "Episode 19/1500, Return = -33.55481727574793, The epsilon now is : 0.6536905161541529\n",
            "Episode 20/1500, Return = -27.79783393501824, The epsilon now is : 0.6375500428128791\n",
            "Episode 21/1500, Return = -34.70790378006884, The epsilon now is : 0.6218080988570593\n",
            "Episode 22/1500, Return = -58.100558659218805, The epsilon now is : 0.6064548440752141\n",
            "Episode 23/1500, Return = -12.621359223300663, The epsilon now is : 0.5914806812235465\n",
            "Episode 24/1500, Return = -30.46357615894084, The epsilon now is : 0.576876250026757\n",
            "Episode 25/1500, Return = -29.10447761194026, The epsilon now is : 0.5626324213269771\n",
            "Episode 26/1500, Return = -40.52044609665442, The epsilon now is : 0.548740291377179\n",
            "Episode 27/1500, Return = -18.604651162790713, The epsilon now is : 0.5351911762754887\n",
            "Episode 28/1500, Return = -10.299003322258834, The epsilon now is : 0.5219766065369207\n",
            "Episode 29/1500, Return = -18.64406779660994, The epsilon now is : 0.5090883217991456\n",
            "Episode 30/1500, Return = -35.1535836177476, The epsilon now is : 0.4965182656589779\n",
            "Episode 31/1500, Return = -32.14285714285759, The epsilon now is : 0.48425858063635735\n",
            "Episode 32/1500, Return = -20.863309352517895, The epsilon now is : 0.47230160326267795\n",
            "Episode 33/1500, Return = -32.475884244373155, The epsilon now is : 0.46063985929039086\n",
            "Episode 34/1500, Return = -24.731182795699542, The epsilon now is : 0.4492660590208893\n",
            "Episode 35/1500, Return = -25.17006802721088, The epsilon now is : 0.4381730927477547\n",
            "Episode 36/1500, Return = -3.703703703703646, The epsilon now is : 0.42735402631251446\n",
            "Episode 37/1500, Return = -6.6147859922181595, The epsilon now is : 0.4168020967701312\n",
            "Episode 38/1500, Return = -17.857142857143515, The epsilon now is : 0.406510708161521\n",
            "Episode 39/1500, Return = -5.594405594405922, The epsilon now is : 0.39647342739045327\n",
            "Episode 40/1500, Return = -28.10457516339934, The epsilon now is : 0.3866839802022521\n",
            "Episode 41/1500, Return = -27.335640138409037, The epsilon now is : 0.37713624726179057\n",
            "Episode 42/1500, Return = -27.835051546392386, The epsilon now is : 0.3678242603283259\n",
            "Episode 43/1500, Return = -29.57746478873305, The epsilon now is : 0.35874219852478123\n",
            "Episode 44/1500, Return = -51.367781155015884, The epsilon now is : 0.3498843846991425\n",
            "Episode 45/1500, Return = -20.689655172414454, The epsilon now is : 0.34124528187570075\n",
            "Episode 46/1500, Return = -19.41391941392011, The epsilon now is : 0.332819489793915\n",
            "Episode 47/1500, Return = -20.86330935251871, The epsilon now is : 0.324601741532736\n",
            "Episode 48/1500, Return = -42.76094276094323, The epsilon now is : 0.3165869002182801\n",
            "Episode 49/1500, Return = -58.188153310105236, The epsilon now is : 0.3087699558127953\n",
            "Episode 50/1500, Return = 64.47368421052838, The epsilon now is : 0.3011460219829101\n",
            "Episode 51/1500, Return = -65.38461538461604, The epsilon now is : 0.2937103330452116\n",
            "Episode 52/1500, Return = -38.77551020408239, The epsilon now is : 0.28645824098724004\n",
            "Episode 53/1500, Return = -57.95795795795843, The epsilon now is : 0.27938521256203835\n",
            "Episode 54/1500, Return = -34.78260869565288, The epsilon now is : 0.27248682645444433\n",
            "Episode 55/1500, Return = -105.05984251968572, The epsilon now is : 0.26685067294282017\n",
            "Episode 56/1500, Return = -51.21951219512239, The epsilon now is : 0.2602617810034442\n",
            "Episode 57/1500, Return = -27.58620689655236, The epsilon now is : 0.25383557741898244\n",
            "Episode 58/1500, Return = -28.825622775801445, The epsilon now is : 0.24756804520128706\n",
            "Episode 59/1500, Return = -23.913043478261546, The epsilon now is : 0.24145526654689972\n",
            "Episode 60/1500, Return = -22.07792207792284, The epsilon now is : 0.2354934203880495\n",
            "Episode 61/1500, Return = 27.58620689655084, The epsilon now is : 0.2296787800041247\n",
            "Episode 62/1500, Return = -35.48387096774268, The epsilon now is : 0.22400771069211617\n",
            "Episode 63/1500, Return = -11.564625850340763, The epsilon now is : 0.21847666749458397\n",
            "Episode 64/1500, Return = -22.55892255892326, The epsilon now is : 0.21308219298372075\n",
            "Episode 65/1500, Return = -49.82078853046672, The epsilon now is : 0.20782091510013148\n",
            "Episode 66/1500, Return = 86.81318681318909, The epsilon now is : 0.2026895450449756\n",
            "Episode 67/1500, Return = -28.327645051195294, The epsilon now is : 0.19768487522415512\n",
            "Episode 68/1500, Return = -6.896551724138567, The epsilon now is : 0.1928037772432628\n",
            "Episode 69/1500, Return = -25.806451612903917, The epsilon now is : 0.18804319995203894\n",
            "Episode 70/1500, Return = -23.875432525952316, The epsilon now is : 0.1834001675371127\n",
            "Episode 71/1500, Return = -17.562724014337608, The epsilon now is : 0.17887177766183437\n",
            "Episode 72/1500, Return = 23.59550561797905, The epsilon now is : 0.17445519965204045\n",
            "Episode 73/1500, Return = 3.58565737051839, The epsilon now is : 0.1701476727266133\n",
            "Episode 74/1500, Return = 94.80519480519916, The epsilon now is : 0.16594650427172947\n",
            "Episode 75/1500, Return = -24.092409240924898, The epsilon now is : 0.16184906815772016\n",
            "Episode 76/1500, Return = 66.6666666666703, The epsilon now is : 0.15785280309749153\n",
            "Episode 77/1500, Return = -61.783439490446604, The epsilon now is : 0.1539552110454759\n",
            "Episode 78/1500, Return = -44.05594405594484, The epsilon now is : 0.15015385563611622\n",
            "Episode 79/1500, Return = -39.40750988141992, The epsilon now is : 0.14710688738119262\n",
            "Episode 80/1500, Return = 118.06853582554973, The epsilon now is : 0.14347462603516137\n",
            "Episode 81/1500, Return = -2.356902356903317, The epsilon now is : 0.13993204996981812\n",
            "Episode 82/1500, Return = 224.81751824817997, The epsilon now is : 0.13647694473834654\n",
            "Episode 83/1500, Return = -20.000000000000757, The epsilon now is : 0.1331071505715171\n",
            "Episode 84/1500, Return = 37.5464684014898, The epsilon now is : 0.12982056102762674\n",
            "Episode 85/1500, Return = 260.71428571428544, The epsilon now is : 0.12661512167577102\n",
            "Episode 86/1500, Return = 34.94809688581257, The epsilon now is : 0.12348882881162927\n",
            "Episode 87/1500, Return = 56.250000000003766, The epsilon now is : 0.12043972820496048\n",
            "Episode 88/1500, Return = 223.432343234325, The epsilon now is : 0.1174659138780229\n",
            "Episode 89/1500, Return = 37.31343283582, The epsilon now is : 0.11456552691415654\n",
            "Episode 90/1500, Return = -23.3226837060711, The epsilon now is : 0.11173675429578372\n",
            "Episode 91/1500, Return = 197.577854671284, The epsilon now is : 0.10897782777110047\n",
            "Episode 92/1500, Return = 386.988847583633, The epsilon now is : 0.10628702274875175\n",
            "Episode 93/1500, Return = 131.2925170068067, The epsilon now is : 0.10366265721979707\n",
            "Episode 94/1500, Return = 417.4825174825103, The epsilon now is : 0.10110309070629554\n",
            "Episode 95/1500, Return = 342.6229508196676, The epsilon now is : 0.098606723235851\n",
            "Episode 96/1500, Return = 520.1550387596832, The epsilon now is : 0.09617199434147725\n",
            "Episode 97/1500, Return = 518.374558303884, The epsilon now is : 0.09379738208615782\n",
            "Episode 98/1500, Return = 351.42857142856553, The epsilon now is : 0.09148140211149056\n",
            "Episode 99/1500, Return = 396.7948717948674, The epsilon now is : 0.0892226067098228\n",
            "Episode 100/1500, Return = 571.3286713286633, The epsilon now is : 0.08701958391929601\n",
            "Episode 101/1500, Return = 585.6060606060518, The epsilon now is : 0.08487095664123553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pylab as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(np.arange(len(rewards)),rewards)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Epsiode #')\n",
        "plt.show()\n",
        "\n",
        "torch.save(agent.qnetwork_target.state_dict(), '/content/drive/MyDrive/ColabNotebooks/CarRacing/weights.pt')"
      ],
      "metadata": {
        "id": "W1QLHizJwGe4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}